{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0027ce42-5007-4596-a8ec-fb4e8d1dc4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/si295/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/si295/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/si295/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/si295/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete! Check the generated CSV files and visualizations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Read the FOMC data\n",
    "df = pd.read_csv('merged_fomc_market_data.csv')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Simple word tokenization\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Process all speeches\n",
    "all_tokens = []\n",
    "for text in df['clean_text']:\n",
    "    tokens = preprocess_text(text)\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "# Create DataFrame of word frequencies\n",
    "freq_df = pd.DataFrame.from_dict(word_freq, orient='index', columns=['frequency'])\n",
    "freq_df.index.name = 'word'\n",
    "freq_df = freq_df.sort_values('frequency', ascending=False)\n",
    "\n",
    "# Save frequency analysis to CSV\n",
    "freq_df.to_csv('word_frequencies.csv')\n",
    "\n",
    "# Create visualizations\n",
    "\n",
    "# 1. Top 20 most frequent words\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(x=freq_df.head(20).index, y='frequency', data=freq_df.head(20))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 20 Most Frequent Words in FOMC Statements')\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_20_words.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Word frequencies by market reaction\n",
    "up_tokens = []\n",
    "down_tokens = []\n",
    "\n",
    "for text, reaction in zip(df['clean_text'], df['market_reaction_up_or_down']):\n",
    "    tokens = preprocess_text(text)\n",
    "    if reaction == 'Up':\n",
    "        up_tokens.extend(tokens)\n",
    "    else:\n",
    "        down_tokens.extend(tokens)\n",
    "\n",
    "up_freq = Counter(up_tokens)\n",
    "down_freq = Counter(down_tokens)\n",
    "\n",
    "# Get top 20 words for each market reaction\n",
    "common_words = list(set(dict(up_freq.most_common(20)).keys()) | set(dict(down_freq.most_common(20)).keys()))\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Up': [up_freq.get(word, 0) for word in common_words],\n",
    "    'Down': [down_freq.get(word, 0) for word in common_words]\n",
    "}, index=common_words)\n",
    "\n",
    "# Normalize frequencies\n",
    "comparison_df['Up'] = comparison_df['Up'] / sum(up_freq.values())\n",
    "comparison_df['Down'] = comparison_df['Down'] / sum(down_freq.values())\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(15, 8))\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('Word Frequencies by Market Reaction (Normalized)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('market_reaction_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# Add market reaction analysis to the frequency CSV\n",
    "freq_df['up_frequency'] = freq_df.index.map(lambda x: up_freq.get(x, 0) / sum(up_freq.values()))\n",
    "freq_df['down_frequency'] = freq_df.index.map(lambda x: down_freq.get(x, 0) / sum(down_freq.values()))\n",
    "freq_df['frequency_difference'] = freq_df['up_frequency'] - freq_df['down_frequency']\n",
    "freq_df.to_csv('word_frequencies_with_market_reaction.csv')\n",
    "\n",
    "print(\"Analysis complete! Check the generated CSV files and visualizations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a403cf-10af-4e08-bd19-1af7db8c7b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
